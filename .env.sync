# BaseVitale - Configuration Neuro-Symbiotique (Mode LOCAL)
# Copiez ces lignes dans votre fichier .env

# Mode IA : LOCAL pour utiliser le sidecar Python (AI Cortex)
AI_MODE=LOCAL

# Service IA (Python / ai-cortex)
# En Docker : http://ai-cortex:8000 (nom du service dans docker-compose)
AI_SERVICE_URL=http://ai-cortex:8000

# Ollama (LLM local)
# Mac + Docker : host.docker.internal pour accéder à Ollama sur la machine hôte
# IMPORTANT : Si Ollama tourne en local (hors Docker) sur Mac, utiliser host.docker.internal
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=llama3.2

# Timeout HTTP (ms) pour appels Cortex. Défaut: 60000 (60s)
# AI_CORTEX_TIMEOUT_MS=60000
