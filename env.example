# BaseVitale - Environment Variables
# Copy to .env and adjust. See also apps/web/.env.local for Next.js.

# Frontend (Next.js) - required at build runtime for client
# NEXT_PUBLIC_API_URL=http://localhost:3000
# NEXT_PUBLIC_WS_URL=http://localhost:3000

# Mode IA (Phase C : Cortex) — MOCK | CLOUD | LOCAL
# MOCK = données factices, pas d'appel Python
# LOCAL = sidecar Python (AI Cortex) sur /process-generic
AI_MODE=LOCAL

# Service IA (Python / ai-cortex) — utilisé quand AI_MODE=LOCAL
# En Docker : le défaut est http://ai-cortex:8000, inutile de changer.
# Pour être explicite :
#   AI_MODE=LOCAL
#   AI_SERVICE_URL=http://ai-cortex:8000
# Hors Docker (dev local, Python sur la machine) :
#   AI_SERVICE_URL=http://localhost:8000
AI_SERVICE_URL=http://ai-cortex:8000
# Alias historique (fallback si AI_SERVICE_URL absent)
AI_CORTEX_URL=http://localhost:8000
# Timeout HTTP (ms) pour appels Cortex. Défaut: 60000 (60s)
# AI_CORTEX_TIMEOUT_MS=60000

# Ollama (LLM local) — utilisé par le sidecar Python (ai-cortex)
# Mac + Docker : host.docker.internal pour accéder à Ollama sur la machine hôte
# docker-compose avec service ollama : http://ollama:11434/v1
# IMPORTANT : Si Ollama tourne en local (hors Docker) sur Mac, utiliser host.docker.internal
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=llama3

# PostgreSQL Configuration
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=basevitale

# Neo4j Configuration
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j

# Redis Configuration (Bull, cache, sémaphore GPU)
REDIS_HOST=localhost
REDIS_PORT=6379
# Leave empty for no password, or set a password
REDIS_PASSWORD=

# Sémaphore GPU (verrou Redis pour appels IA en mode LOCAL)
# GPU_LOCK_TTL_SECONDS=120
# GPU_LOCK_MAX_WAIT_MS=60000

# MinIO Configuration
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
