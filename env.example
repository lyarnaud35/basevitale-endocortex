# BaseVitale - Environment Variables
# Copy to .env and adjust. See also apps/web/.env.local for Next.js.

# Frontend (Next.js) - required at build runtime for client
# API NestJS (Scribe). En dev : Web sur 3000, API sur 3001 → définir les deux.
# NEXT_PUBLIC_API_URL=http://localhost:3001
# NEXT_PUBLIC_WS_URL=http://localhost:3000

# API (NestJS) - port. En dev local avec Web (3000), utiliser 3001 pour éviter conflit.
# PORT=3001

# Next.js rewrites : cible du proxy /api → API NestJS (défaut http://localhost:3001)
# API_BACKEND_URL=http://localhost:3001

# Mode IA (Phase C : Cortex) — MOCK | CLOUD | LOCAL
# MOCK = données factices, pas d'appel Python
# LOCAL = sidecar Python (AI Cortex) sur /process
# CLOUD = Groq ou OpenAI (réponse < 2s, idéal pour dev UI)
# Phase 3 démo : AI_MODE=CLOUD + GROQ_API_KEY ou OPENAI_API_KEY pour test réel rapide
AI_MODE=LOCAL

# CLOUD (AI_MODE=CLOUD) — Groq ou OpenAI — Phase 3 démo "Démonstration de puissance"
# CLOUD_LLM_PROVIDER=groq | openai (défaut: groq si GROQ_API_KEY, sinon openai)
# GROQ_API_KEY=gsk_...
# OPENAI_API_KEY=sk-...
# GROQ_MODEL=llama-3.1-8b-instant (défaut)
# OPENAI_MODEL=gpt-3.5-turbo (défaut)

# Service IA (Python / ai-cortex) — utilisé quand AI_MODE=LOCAL
# En Docker : le défaut est http://ai-cortex:8000, inutile de changer.
# Pour être explicite :
#   AI_MODE=LOCAL
#   AI_SERVICE_URL=http://ai-cortex:8000
# Hors Docker (dev local, Python sur la machine) :
#   AI_SERVICE_URL=http://localhost:8000
AI_SERVICE_URL=http://ai-cortex:8000
# Alias historique (fallback si AI_SERVICE_URL absent)
AI_CORTEX_URL=http://localhost:8000
# Timeout HTTP (ms) pour appels Cortex. Défaut: 60000 (60s)
# AI_CORTEX_TIMEOUT_MS=60000

# Ollama (LLM local) — utilisé par le sidecar Python (ai-cortex)
# Mac + Docker : host.docker.internal pour accéder à Ollama sur la machine hôte
# docker-compose avec service ollama : http://ollama:11434/v1
# IMPORTANT : Si Ollama tourne en local (hors Docker) sur Mac, utiliser host.docker.internal
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=llama3

# PostgreSQL Configuration
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=basevitale

# Neo4j Configuration
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j

# Redis Configuration (Bull, cache, sémaphore GPU)
REDIS_HOST=localhost
REDIS_PORT=6379
# Leave empty for no password, or set a password
REDIS_PASSWORD=

# Sémaphore GPU (verrou Redis pour appels IA en mode LOCAL)
# GPU_LOCK_TTL_SECONDS=120
# GPU_LOCK_MAX_WAIT_MS=60000

# MinIO Configuration
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Sécurité API (consommation backend partenaire / Hostile Environment)
# Origines CORS autorisées (séparées par des virgules). Vide = aucune ; non défini = toutes (dev).
# ALLOWED_ORIGINS=https://app.partenaire.com,https://admin.partenaire.com
# Clé API pour authentification backend-to-backend (header X-INTERNAL-API-KEY). Si vide, le guard n'exige pas de clé.
# INTERNAL_API_KEY=your-secret-api-key
